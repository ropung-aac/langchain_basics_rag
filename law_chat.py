import streamlit as st
import os
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="ë²•ë¥ ê°€ ì±—ë´‡", page_icon=":books:", layout="wide")

st.title("ğŸ“š ë²•ë¥ ê°€ ì±—ë´‡")
st.caption("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:\nOPENAI_API_KEY=your_api_key_here")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
@st.cache_resource
def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        # PDF ë¬¸ì„œ ë¡œë”©
        loader = PyPDFLoader("tax.pdf")
        documents = loader.load()
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-multitask"
        )
        
        # ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        
        return vectorstore
    except Exception as e:
        st.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜
def get_rag_response(question, vectorstore, api_key):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ì„¸ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}), # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        response = qa_chain.run(question)
        return response
        
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    with st.spinner("PDF ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        st.session_state.vectorstore = initialize_rag_system()

# ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input(placeholder="ì„¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”..."):
    # API í‚¤ í™•ì¸
    if not openai_api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    elif st.session_state.vectorstore is None:
        st.error("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        with st.chat_message("user"):
            st.write(user_question)
        st.session_state.messages.append({"role": "user", "content": user_question})

        # AI ë‹µë³€ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                ai_response = get_rag_response(
                    user_question, 
                    st.session_state.vectorstore, 
                    openai_api_key
                )
            st.write(ai_response)
        st.session_state.messages.append({"role": "assistant", "content": ai_response})

# ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ"):
    st.write("ì„¸ì…˜ ìƒíƒœ:", st.session_state.messages)